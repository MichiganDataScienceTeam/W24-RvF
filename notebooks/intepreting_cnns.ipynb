{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ_R9e6ErVxy"
      },
      "source": [
        "# Interpreting CNNs\n",
        "\n",
        "While deep learning, particularly CNNs, has achieved remarkable success across diverse applications like self-driving cars and facial recognition, a persistent lack of trust remains in their decision-making processes. Improving CNN interpretability is crucial for enhancing model performance and reliability.\n",
        "\n",
        "This notebook focuses on introducing the Gradient-weighted Class Activation Mapping (Grad-CAM) technique, promising to enhance model interpretability and facilitate more transparent and trustworthy CNNs, leading to improved training strategies and model generalization across various applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvZVEQYWrcO4"
      },
      "source": [
        "The following cell runs setup code needed to get the Kaggle dataset for use in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUkWQMburZEK"
      },
      "outputs": [],
      "source": [
        "!pip -q install grad-cam\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change this to the folder containing your Kaggle API key (kaggle.json)\n",
        "%env KAGGLE_KEY_FOLDER=MDST/RvF\n",
        "!mkdir data\n",
        "!export KAGGLE_CONFIG_DIR=/content/drive/MyDrive/$KAGGLE_KEY_FOLDER && wget -O - \"https://raw.githubusercontent.com/MichiganDataScienceTeam/W24-RvF/main/data/download.sh\" | bash -s rvf10k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaA6QDqQrVx0"
      },
      "source": [
        "## Grad-CAM\n",
        "\n",
        "Grad-CAM was developed in 2016 by researchers as a method to visualize _what_ a convolutional neural network is paying attention to when it makes a certain prediction. This can be useful - if the CNN is paying attention to the wrong things, then it would imply the model is not learning the correct patterns and trends, and that work needs to be done to combat the generalization error.\n",
        "\n",
        "The `pytorch-grad-cam` package offers a neat implementation for producing Grad-CAM visualizations for any convolutional neural network implemented in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FYbtbwPvZYi"
      },
      "source": [
        "To get started, load a trained model into your notebook. For this example, we will use the `model_4.pt` file uploaded to the [media](https://github.com/MichiganDataScienceTeam/W24-RvF/blob/main/media) folder from GitHub, download it to this notebook environment. The following cell does this for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38jSXPNxsJOK",
        "outputId": "0eececd2-ce80-48e6-f953-9836708fbc7d"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/MichiganDataScienceTeam/W24-RvF/raw/main/media/model_4.pt\n",
        "!mkdir -p checkpoints/Net && mv model_4.pt checkpoints/Net\n",
        "!wget https://raw.githubusercontent.com/MichiganDataScienceTeam/W24-RvF/main/starter_code/train.py\n",
        "\n",
        "import torch\n",
        "from train import load_model\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Constructor for the neural network.\"\"\"\n",
        "        super(Net, self).__init__()        # Call superclass constructor\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = torch.nn.Conv2d(in_channels=32, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.fc = torch.nn.Linear(2048, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z1 = self.conv1(x)\n",
        "        h1 = self.relu(z1)\n",
        "        p1 = self.pool(h1)\n",
        "\n",
        "        z2 = self.conv2(p1)\n",
        "        h2 = self.relu(z2)\n",
        "        p2 = self.pool(h2)\n",
        "\n",
        "        z3 = self.conv3(p2)\n",
        "        h3 = self.relu(z3)\n",
        "        p3 = self.pool(h3)\n",
        "\n",
        "        z4 = self.conv4(p3)\n",
        "        h4 = self.relu(z4)\n",
        "        p4 = self.pool(h4)\n",
        "\n",
        "        flat = self.flatten(p4)\n",
        "        z = self.fc(flat)\n",
        "\n",
        "        return z\n",
        "\n",
        "model = Net()\n",
        "load_model(model, \"checkpoints\", 4, map_location=\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj8_onV_-G8E"
      },
      "source": [
        "ðŸš¨ **CRITICAL** ðŸš¨: For this tutorial, we downloaded the weights that were from training the architecture defined. For your model, you will have to load the model that"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMQmzhLywpkA"
      },
      "source": [
        "From here, running visualizations on the dataset isn't very difficult! The following cell will:\n",
        "- Load 100 images of fake faces\n",
        "- Use our model to evaluate what the model pays attention to in _each_ of those fake faces when training the image via the Grad-CAM method\n",
        "- Visualize these regions on the input image to give us intuition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmO5oJXTwkBN",
        "outputId": "39fce4e6-49b5-4f5c-87e6-8c927d601011"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as tf\n",
        "\n",
        "visualizations = []\n",
        "cam = GradCAM(model=model, target_layers=[model.conv2, model.conv3])\n",
        "\n",
        "# Set Class 0 = figure out what parts of the image make the model think face is fake\n",
        "targets = [ClassifierOutputTarget(0)]\n",
        "\n",
        "for image_path in tqdm(Path(\"./data/rvf10k/train/fake\").iterdir()):\n",
        "    image = cv2.imread(str(image_path))\n",
        "    pipeline = tf.Compose([\n",
        "        tf.ToTensor(),\n",
        "        tf.ConvertImageDtype(torch.float32),\n",
        "    ])\n",
        "    image = np.float32(image) / 255\n",
        "    input_tensor = pipeline(image).view(1,3,256, 256)\n",
        "\n",
        "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "    visualization = show_cam_on_image(image, grayscale_cam[0, :], use_rgb=True)\n",
        "    visualizations.append(visualization)\n",
        "    if len(visualizations) == 100:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro-hX15zyTwm"
      },
      "source": [
        "We can now take a look at the visualized images to see patterns the model is paying attention to predict a fake face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "juyAHahow2vD",
        "outputId": "594e6fa8-4d73-456e-b014-81bcb7f4d4a9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(visualizations[23])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ3Y53B0y_H2"
      },
      "source": [
        "The lighter the color, the more the model pays attention to that part of the input image. In the example above, the model seems to be paying attention mainly to the forehead and the nose - which seems reasonable for identifying a fake face.\n",
        "\n",
        "However GradCAM can also show that sometimes the model doesn't always try and fit to the right trends!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "NYDUaePyy7qe",
        "outputId": "8555ce35-00bf-4e4a-8de0-9548775c6a85"
      },
      "outputs": [],
      "source": [
        "plt.imshow(visualizations[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-WQ5iElzqiH"
      },
      "source": [
        "This visualization is more interesting - for some reason, our model has identified the background as something that may influence a fake face ... looks like this model is **overfitting**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "8GMGG1bx4fu2",
        "outputId": "2b71cf8a-b085-463d-b49d-c0120790bd4b"
      },
      "outputs": [],
      "source": [
        "plt.imshow(visualizations[30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiERVUBz-OT5"
      },
      "source": [
        "Here to, the model seems to be looking more at the background and the edge of the image rather than the actual face - this also might indicate overfitting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DK6WVQh0DhX"
      },
      "source": [
        "Explore some more! Can you find some more generic trends in how the model is overfitting (if at all?) And can you adapt this to your _own_ model to determine what your model is learning?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
