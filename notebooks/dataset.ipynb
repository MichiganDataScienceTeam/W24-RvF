{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajp8OHfeKi5f"
      },
      "source": [
        "# Datasets and PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEi07RLTKi5l"
      },
      "source": [
        "As we saw during the [PyTorch Tutorial](https://github.com/MichiganDataScienceTeam/W24-RvF/blob/main/notebooks/pytorch_cnn.ipynb), dataset management is a large part of training neural networks.\n",
        "\n",
        "This notebook will walk you through the fundamentals of how to create _custom_ datasets in PyTorch, and will give you an example of a custom dataset for PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FYM9-ACKi5l"
      },
      "source": [
        "## Extending `Dataset`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql1kRnwDKi5l"
      },
      "source": [
        "The `Dataset` ([docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)) class is the fundamental building block for all datasets. It represents a collection of data to use in training deep learning models. At their surface, `Dataset` looks similar to a Python list, but can be customized extensively for better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OsswYqzKi5m"
      },
      "source": [
        "Below we'll create an _outline_ for a custom dataset to give you an idea for what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QRSZ5lt4Ki5m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class RvFDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self, index) -> tuple[torch.Tensor, int]:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0nOaJBDKi5n"
      },
      "source": [
        "Let's break down what's happening the code above by step\n",
        "1. **Define the custom dataset** - The first line says that `RvFDataset` is a _type_ of dataset that follows the requirements of the original `torch.utils.data.Dataset`\n",
        "   ```python\n",
        "   class RvFDataset(torch.utils.data.Dataset)\n",
        "   ```\n",
        "2. **Create a constructor** - The `__init__` function tells Python _how_ to create the dataset. Here is where we would\n",
        "    - define any necessary variables (for example, metadata about the images in our dataset)\n",
        "    - create any **image preprocessing** functions (for example, cropping or grayscaling images)\n",
        "3. **Define how to fetch an training example** - The `__getitem__` function tells PyTorch how to get a **training example** from the dataset. We define a training example as the combination of an image and a label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppTrfCOjKi5n"
      },
      "source": [
        "That's all! As long as you implement the `__init__` and `__getitem__` functions, you will have your own dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XIuPG2-Ki5o"
      },
      "source": [
        "### Offical RvF Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VIlmtiMKi5o"
      },
      "source": [
        "Of course, the example above is very limited with no actual code. Below we have a full example - this is the dataset that you will be using to load all the data for this project!\n",
        "\n",
        "We didn't have you implement all of the code for this dataset as some of the concepts are advanced and add extra complexity to an already challenging project.\n",
        "\n",
        "That said - If you have any questions about what the code above is doing, feel free to ask the project leads!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5OvnRnd0Ki5o"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Union, Callable\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy.typing as npt\n",
        "import torchvision\n",
        "import imageio.v3 as iio\n",
        "\n",
        "class RvFDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            split: str = \"train\",\n",
        "            data_directory : Union[str, Path] = \"data/rvf10k\",\n",
        "            preprocessor : Callable[[npt.ArrayLike], torch.Tensor] = None\n",
        "    ):\n",
        "        self.data_directory = Path(data_directory)\n",
        "        self.metadata = pd.read_csv(self.data_directory / f\"{split}.csv\")\n",
        "\n",
        "        if preprocessor is None:\n",
        "            self.preprocessor = torchvision.transforms.ToTensor()\n",
        "        else:\n",
        "            self.preprocessor = preprocessor\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, index) -> tuple[torch.Tensor, int]:\n",
        "        image_metadata = self.metadata.iloc[index]\n",
        "        path = self.data_directory / image_metadata[\"path\"]\n",
        "        image = self.preprocessor(iio.imread(path))\n",
        "        return image, image_metadata[\"label\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA-ap_4eKi5o"
      },
      "source": [
        "## Batching and Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb6itRAdKi5p"
      },
      "source": [
        "As mentioned in the PyTorch tutorial, we often want to train a (supervised) machine learning model using an algorithm called [mini-batch SGD](https://developers.google.com/machine-learning/crash-course/reducing-loss/stochastic-gradient-descent)\n",
        "\n",
        "PyTorch provides the `DataLoader` ([docs](https://pytorch.org/docs/stable/data.html?highlight=data+loader#torch.utils.data.DataLoader)) to prepare data into batches for training neural networks.\n",
        "\n",
        "Let's see an example of a data loader in practice!\n",
        "\n",
        "ðŸš¨ **WARNING** ðŸš¨: The following code will not work unless you have the `rvf10k` dataset downloaded. The code below will download the dataset if you do not have it downloaded already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxujXq3MKi5p"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change this to the folder containing your Kaggle API key (kaggle.json)\n",
        "%env KAGGLE_KEY_FOLDER=MDST/RvF\n",
        "!mkdir data\n",
        "!export KAGGLE_CONFIG_DIR=/content/drive/MyDrive/$KAGGLE_KEY_FOLDER && wget -O - \"https://raw.githubusercontent.com/MichiganDataScienceTeam/W24-RvF/main/data/download.sh\" | bash -s rvf10k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqIAR-xOKi5p"
      },
      "source": [
        "Let's create an instance of our RvF Dataset below and wrap it in a `DataLoader` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eJZUsUD2Ki5q"
      },
      "outputs": [],
      "source": [
        "train_dataset = RvFDataset(\"train\", data_directory=\"data/rvf10k\")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_dataset = RvFDataset(\"valid\", data_directory=\"data/rvf10k\")\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTDh2ZivKi5q"
      },
      "source": [
        "Let's break down the settings we just passed in to create the DataLoader:\n",
        "- `train_dataset` - this is the dataset object we want to wrap and load batches from\n",
        "- `batch_size` - this describes how many training examples we'd like to include in a single batch\n",
        "- `shuffle` - tells the data loader to _shuffle_ the images when selecting them. Keep this enabled for training data loaders, but disable this for testing data loaders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zESj_saKi5q"
      },
      "source": [
        "If we take a look at the data loaders, we can see indeed that the loaded data contains a _batch_ of training examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "491uzGXvKi5q",
        "outputId": "aadb5758-52f1-4f47-dca1-62ea04607f3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of batch of images: torch.Size([32, 3, 256, 256])\n",
            "Shape of batch of labels: torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(train_loader)) # don't worry about this magic for now\n",
        "print(f\"Shape of batch of images: {batch[0].shape}\")\n",
        "print(f\"Shape of batch of labels: {batch[1].shape}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
