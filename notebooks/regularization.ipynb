{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGmV3UdiSVAt"
      },
      "source": [
        "# Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcyP52_2SVAz"
      },
      "source": [
        "As we saw with the CNN competition, CNNs have the tendency to **overfit** to their dataset, where our CNNs had:\n",
        "- high performance on the training dataset\n",
        "- low performance on our validation dataset\n",
        "\n",
        "But what does this mean? And how do we fight it?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEG8jpGASVAz"
      },
      "source": [
        "## Variance and Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOZU1XaVSVAz"
      },
      "source": [
        "**Variance** is a characteristic of a machine learning model that describes how _variant_ a model based on the dataset it's trained on. A high variance model is one that is _super_ sensitive to input training data provided - literally removing a _single_ data point could change the trends learned by the model!\n",
        "\n",
        "Models that are super complex (such as CNNs) often have high variance because their complexity allows them to learn super complicated patterns in the training data. This makes them _very_ sensitive to the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1adbn7PakUr"
      },
      "source": [
        "A **high variance** model is not a problem on its own! However when a high variance model is combined with either:\n",
        "- a dataset that is too small\n",
        "- a noisy dataset which contains lots of outliers / trends that don't generalize well to all data points\n",
        "\n",
        "we observe the problem called **overfitting** - a situation where a model hyperfixates on noisy trends in the training data and fails to generalize to new data in the wild. As a result, an overfitting model has strong training set performance but _very_ poor testing set performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNXXeB-0b0JH"
      },
      "source": [
        "To see an example of overfitting, consider the case study from [this popular research paper](https://arxiv.org/pdf/1807.04975.pdf). In the paper, the researchers trained a computer vision model to recognize entities (such as a cow ðŸ®).\n",
        "\n",
        "The authors then tried to extend their model to novel situations - such as identifying a cow on the beach (ðŸ® ðŸï¸). For each of 3 situations, the authors, compared the top 5 most likely classes as per the model's predictions, and reported the results below (image reproduced from Figure 1 in paper).\n",
        "\n",
        "![Image of machine learning model training results for a cow in a pasture vs a cow on a beach.](https://miro.medium.com/v2/resize:fit:1400/0*EWVAp3dh3d5YwsGn.png)\n",
        "\n",
        "The figure above shows a clear trend:\n",
        "- for cows that appear in pastures, which is a common scenario, the model is very confident that the image contains a cow (99% confident)\n",
        "- for cows that appear on a beach, the model has no clue that the image contains a cow\n",
        "\n",
        "Why is this happening? **Overfitting**! The model in paper was trained _only_ on data that contained cows on pastures (as most cows usually do live on pastures). However as a result, the model learned to associate _pastures_ with _cows_ - which is a noisy trend! While cows may certainly tend to live on pastures, a cow need not live on a pasture to be a cow.\n",
        "\n",
        "In other words, the model fit _too closely_ to noisy patterns in the training dataset (the presence of pastures for cows) and learned relationships that didn't generalize well to new data (like cows on the beach).\n",
        "\n",
        "**KEY ðŸ”‘**: It's easy to write this off as a strange edge case, but overfitting is _extremely common_ for convolutional neural networks. These are really powerful models that very easily overfit to training data unless you have particularly clean data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgzFne1kSVA0"
      },
      "source": [
        "## Reducing Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_xHWoT9SVA0"
      },
      "source": [
        "Let's work through an example that will help us see how we can use deep learning methods to reduce the amount of overfitting. Let's start by training a super simple CNN on the [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset containing images of items of clothing.\n",
        "\n",
        "Let's start by training the CNN on the dataset and observing the results! This will take about 4 minutes to complete.\n",
        "\n",
        "------\n",
        "\n",
        "**Hint**: Enable GPUs to speed this up! Select `Runtime > Change runtime type` above to switch to \"T4 GPU\". **CRITICAL** - make sure to switch back to CPU when done with this notebook (otherwise you might get banned from Google Colab ðŸ˜¬)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPOsOBbuSVA0"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "converter = ToTensor()\n",
        "\n",
        "train = datasets.FashionMNIST(root=\"./private\", download=True, train=True, transform=converter)\n",
        "test = datasets.FashionMNIST(root=\"./private\", download=True, train=False, transform=converter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yItCJoSSVA1"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(144*4, 10)\n",
        "\n",
        "    def forward(self, img):\n",
        "        x1 = self.pool(self.relu(self.conv1(img)))\n",
        "        x2 = self.pool(self.relu(self.conv2(x1)))\n",
        "        x3 = self.pool(self.relu(self.conv3(x2)))\n",
        "        flat = self.flatten(x3)\n",
        "        return self.dense(flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sADY7Sh5WIll"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "seaborn.set_theme()\n",
        "\n",
        "def evaluate(\n",
        "    model: torch.nn.Module, criterion: Callable, loader: DataLoader\n",
        ") -> tuple[float]:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        loss = 0.0\n",
        "        for X, y in loader:\n",
        "            outputs = model(X.to(device)).to(\"cpu\")\n",
        "            loss += criterion(outputs, y).detach().sum().item()\n",
        "            _, predicted = torch.max(outputs.data, 1)  # get predicted digit\n",
        "            total += len(y)\n",
        "            correct += (predicted == y).sum().item()\n",
        "    model.train()\n",
        "    return correct / total, loss / total\n",
        "\n",
        "def train_model(\n",
        "    model: torch.nn.Module,\n",
        "    criterion: Callable,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    train_loader: DataLoader,\n",
        "    test_loader: DataLoader,\n",
        "    epochs: int = 10,\n",
        ") -> dict[str, list[float]]:\n",
        "    train_losses, train_accuracies = [], []\n",
        "    test_losses, test_accuracies = [], []\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "\n",
        "        for X, y in tqdm(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X.to(device))\n",
        "            loss = criterion(outputs, y.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_accuracy, train_loss = evaluate(model, criterion, train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        test_accuracy, test_loss = evaluate(model, criterion, test_loader)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}: Loss - (Train {train_loss:.2f}/Test {test_loss:.2f}, \"\n",
        "            f\"Accuracy - (Train {train_accuracy:.2f}/Test {test_accuracy:.2f})\"\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"loss\": {\n",
        "            \"train\": train_losses,\n",
        "            \"test\": test_losses,\n",
        "        },\n",
        "        \"accuracy\": {\n",
        "            \"train\": train_accuracies,\n",
        "            \"test\": test_accuracies,\n",
        "        },\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_performance(history: dict[str, dict[str, list[float]]]) -> mpl.figure.Figure:\n",
        "    fig, axes = plt.subplots(len(history), 1, figsize=(10, 8))\n",
        "    for i, (metric, values) in enumerate(history.items()):\n",
        "        train, test = values[\"train\"], values[\"test\"]\n",
        "        axes[i].plot(train, label=\"train\")\n",
        "        axes[i].plot(test, label=\"test\")\n",
        "        axes[i].set_title(f\"{metric}\")\n",
        "        axes[i].set_xlabel(\"Epoch\")\n",
        "        axes[i].set_ylabel(metric)\n",
        "        axes[i].legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-tRUlN1SVA1",
        "outputId": "f62fef5e-0410-4803-9b29-5bc21a630436"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train, batch_size=32)\n",
        "test_loader = DataLoader(test, batch_size=32)\n",
        "\n",
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
        "\n",
        "history = train_model(model, criterion, optimizer, train_loader, test_loader, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF4nLAv6SVA2"
      },
      "source": [
        "Now let's plot our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "CrxJzHNzSVA2",
        "outputId": "f1a1c6b3-8339-4721-d74d-ce02ad0ad537"
      },
      "outputs": [],
      "source": [
        "plot_performance(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE6WtOB9gx-3"
      },
      "source": [
        "Notice the gap between the training and testing performance - that's a clear sign of overfitting (as our model is doing better on the training set than on the testing set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfJuXbqnSVA2"
      },
      "source": [
        "Given that our model is overfitting, let's go over ways to reduce the amount of model overfitting. There are three ways (among many others) with which we will try and do so:\n",
        "1. Reduce model complexity\n",
        "2. Add dropout\n",
        "3. Add weight decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GmFp2unSVA3"
      },
      "source": [
        "### Reducing Model Complexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoURZRaUaXXO"
      },
      "source": [
        "We say that a model is **complex** when it can learn more complicated patterns. For a CNN, model complexity is mainly a function of the number of layers and number of filters.\n",
        "\n",
        "As a less complex model is less prone to overfitting, lets define a new CNN architecture that reduces the number of filters and see how our performance changes:\n",
        "\n",
        "| Layer | Old Filter Count | New Filter Count |\n",
        "| ----- | ---------------- | ---------------- |\n",
        "| 1     | 16               | 8                |\n",
        "| 2     | 32               | 8                |\n",
        "| 3     | 64               | 16               |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QobnqslIaNKR"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class LessComplexCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LessComplexCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(8, 8, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(144, 10)\n",
        "\n",
        "    def forward(self, img):\n",
        "        x1 = self.pool(self.relu(self.conv1(img)))\n",
        "        x2 = self.pool(self.relu(self.conv2(x1)))\n",
        "        x3 = self.pool(self.relu(self.conv3(x2)))\n",
        "        flat = self.flatten(x3)\n",
        "        return self.dense(flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVy79iPTbWtS",
        "outputId": "3d07d781-c04b-4bf7-8bfc-2649f0f1bc91"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train, batch_size=32)\n",
        "test_loader = DataLoader(test, batch_size=32)\n",
        "\n",
        "model = LessComplexCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
        "\n",
        "history = train_model(model, criterion, optimizer, train_loader, test_loader, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "BMocXmzUbaxd",
        "outputId": "5b0877a4-6999-4b89-af04-bfc1f1bc597f"
      },
      "outputs": [],
      "source": [
        "plot_performance(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRmcdQW5cStG"
      },
      "source": [
        "Now notice how the gap is smaller! Our model now overfits less to the training data (of course, at the cost of worse train accuracy).\n",
        "\n",
        "**KEY ðŸ”‘**: This is a downside of reducing model complexity - by reducing the model complexity, you reduce the ability for the model to learn trends in the dataset, reducing training set performance. This is called the **bias-variance tradeoff**, and is a key problem in machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNCl-kzocaVx"
      },
      "source": [
        "### Adding Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTDibKkNcdyO"
      },
      "source": [
        "Another way to reduce model complexity (than just reducing the number of filters) is to add something called **dropout** as a layer in your model.\n",
        "\n",
        "Dropout is a special layer that just takes the inputs, and **randomly** delets some percentage of the values by setting them to 0. Deleting inputs seems counterintuitive, but the fact that dropout is done _randomly_ forces the most to pay less attention to noise.\n",
        "\n",
        "If you want to learn more, checkout the original [research paper](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sZb3u19c_x3"
      },
      "source": [
        "The cell below adds dropout to our original CNN architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnekpSB9cT8N"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class DropoutCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DropoutCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(144*4, 10)\n",
        "\n",
        "        # this is new!\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, img):\n",
        "        x1 = self.pool(self.relu(self.conv1(img)))\n",
        "        x2 = self.pool(self.relu(self.conv2(x1)))\n",
        "        x3 = self.pool(self.relu(self.conv3(x2)))\n",
        "        flat = self.flatten(x3)\n",
        "        flat_droppedout = self.dropout(flat) # we usually apply dropout to linear layers only\n",
        "        return self.dense(flat_droppedout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXdrokPMdZY-",
        "outputId": "8ea89a96-b520-4fae-c6f6-71f2d64595b9"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train, batch_size=32)\n",
        "test_loader = DataLoader(test, batch_size=32)\n",
        "\n",
        "model = DropoutCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
        "\n",
        "history = train_model(model, criterion, optimizer, train_loader, test_loader, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "-MCgpRkadbtS",
        "outputId": "984f50cd-df1b-43fe-88d2-97c66d6c91b2"
      },
      "outputs": [],
      "source": [
        "plot_performance(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DdrKe1ZefZr"
      },
      "source": [
        "Here too - we see the gap between training and testing loss decrease! Except now, we don't give up as much training accuracy to reduce overfitting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbSAKbDKeq-s"
      },
      "source": [
        "### Adding Weight Decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTKtV28keuo8"
      },
      "source": [
        "Yet another way to reduce model complexity (than just reducing the number of filters) is to use something called **weight decay**.\n",
        "\n",
        "Unlike dropout, which is its own special layer in a neural network, weight decay is part of the _optimizer_. It essentially tells the optimizer to add a penalty if the model is too complex. Therefore when training the CNN, we make sure to train a model that has lower complexity.\n",
        "\n",
        "The cell below edits the definition of our optimizer to add weight decay. We will use `AdamW` instead of `Adam` now to make sure weight decay works properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eJtufWMetGV",
        "outputId": "666aea64-f99e-49cb-fd98-679f53b0b267"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train, batch_size=32)\n",
        "test_loader = DataLoader(test, batch_size=32)\n",
        "\n",
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=0.1) # this is new!\n",
        "\n",
        "history = train_model(model, criterion, optimizer, train_loader, test_loader, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "v4r-AYrdgfPJ",
        "outputId": "cfe79bec-c9db-47cf-cd25-922cd2101bb7"
      },
      "outputs": [],
      "source": [
        "plot_performance(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3tg_X5SuUl7"
      },
      "source": [
        "Yet again, we see that a smaller gap between the train and validation curves, showing that the amount of overfitting is decreasing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
