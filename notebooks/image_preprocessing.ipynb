{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rro6Rqd3lDYy"
      },
      "source": [
        "# Image Preprocessing\n",
        "\n",
        "Convolutional networks take images as inputs - but not all images are in the right format for training convolutional neural networks. As a result, we need to _preprocess_ our images before using them to training convolutional neural networks. This notebook covers preprocessing methods that we can use to prepare images for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jq1piSLwyxj"
      },
      "source": [
        "## Preprocessing Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5RBTPZX3yiu"
      },
      "source": [
        "For this section, we will use this image of a lovely dog üê∂ to show examples of image preprocessing methods:\n",
        "\n",
        "![Image of a lovely dog](https://github.com/pytorch/vision/blob/main/gallery/assets/dog2.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gwCbecJ40rK",
        "outputId": "73b4d150-8a3f-413a-eed6-13df19bc2c54"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/pytorch/vision/main/gallery/assets/dog2.jpg\n",
        "import imageio.v3 as iio\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def plot(*args):\n",
        "    num_imgs = len(args)\n",
        "    fig, axes = plt.subplots(1, num_imgs, figsize=(4 * num_imgs, 4))\n",
        "    if num_imgs == 0:\n",
        "        raise ValueError(\"Plot function received no plots\")\n",
        "    elif num_imgs  == 1:\n",
        "        axes.imshow(args[0].permute(1,2,0))\n",
        "    else:\n",
        "        for index, arg in enumerate(args):\n",
        "            axes[index].imshow(arg.permute(1,2,0))\n",
        "            axes[index].set_title(f\"Image {index}\")\n",
        "            axes[index].xaxis.set_ticklabels([])\n",
        "            axes[index].yaxis.set_ticklabels([])\n",
        "\n",
        "image = torch.from_numpy(iio.imread(\"dog2.jpg\")).permute(2,0,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBJlw4qq_5bw"
      },
      "source": [
        "**NOTE**: The last `.permute()` call is important!\n",
        "- In many Python libraries like `imageio` and `matplotlib`, it is assumed that images are of the format $H \\times W \\times C$ (channel is the last dimension)\n",
        "- In PyTorch, it is assumed that images are of the format $C \\times H \\times W$ (channel is the first dimension)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zbwnU1Aur1v"
      },
      "source": [
        "### Standardization and Normalization\n",
        "\n",
        "For a bunch of mathematical reasons, we'd prefer to have two preferable qualities when training neural networks (or any machine learning algorithm in general):\n",
        "- **all features are on the same scale** - for example, we want all features to lie in the range [0, 1] or have a mean 1 with a fixed standard deviation\n",
        "- **all features have relatively small values** - we don't want features with large values such as 1000 or -10232\n",
        "\n",
        "For this reason, we often either **standardize** or **normalize** input features.\n",
        "- **standardize** - transform data so all values lie in the range $[0, 1]$\n",
        "- **normalize** - transform each feature so the mean of the feature is 1 and the standard deviation from the mean is 0. More aggressive in bringing outliers closer to mean than standardization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZh4nd0KxWyR"
      },
      "source": [
        "To perform standardization, it suffices to divide the image's pixels by 255. This is because a pixel can have a value of 255 at max. We should see nothing has changed in the image as all we have done is scale the values down to fit in the range $[0,1]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "iL-lntc-lAe6",
        "outputId": "406d7087-e067-41b2-a5ca-e26e94c8fd4b"
      },
      "outputs": [],
      "source": [
        "standardized_image = image / 255\n",
        "plot(image, standardized_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwWxSyYm5fzc"
      },
      "source": [
        "To normalize an image, we look at the entire _training_ dataset and compute:\n",
        "- the mean $\\mu_R$ and standard deviation $\\sigma_R$ of the red channel\n",
        "- the mean $\\mu_G$ and standard deviation $\\sigma_G$ of the green channel\n",
        "- the mean $\\mu_B$ and standard deviation $\\sigma_B$ of the blue channel\n",
        "\n",
        "with which we can compute the z-scores for each pixel in each channel. If $x_{cij}$ is the pixel in channel $c$, row $i$, and column $j$, the corresponding output pixel after normalization will be $$z_{cij} = \\frac{x_{cij}-\\mu_c}{\\sigma_c}$$\n",
        "\n",
        "Luckily the `torchvision` package handles this transformation for us!\n",
        "\n",
        "**NOTE**: In the code below, the normalization assumes that the image is already standardized. This is necessary as z-scores can be any real number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "0WehzTD77Kny",
        "outputId": "8677f496-6e38-4fe9-beda-cefa3eba272e"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.v2 as v2\n",
        "\n",
        "# this mean and standard deviation is found from the ImageNet dataset\n",
        "# in practice you'd have to find the mean and standard deviation for your own dataset (like the RvF dataset)\n",
        "normalizer = v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "normalized_image = normalizer(standardized_image.to(torch.float32))\n",
        "plot(image, normalized_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwsxysC0DqaC"
      },
      "source": [
        "We see some difference in the images now as some pixels become negative after computing the z-score. That said, note how the original structure is preserved - this is more critical for allowing our machine learning algorithm to learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTbhnGzDxwOg"
      },
      "source": [
        "Here is some theory if you're interested - but it's not at all needed to work on this project.\n",
        "\n",
        "> Training neural networks (and machine learning models in general) use iterative descent methods like [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) to minimize a loss heuristic. These methods use local approximators (such as the [gradient](https://en.wikipedia.org/wiki/Gradient) of a multivariate surface) to determine which direction to move in to minimize the loss function. This leads to two major challenges, both of which can be solved through either standardization or normalization\n",
        "> - If features are on different scales, then some parts of the loss surface will be steeper than others (see contour plot below). As gradient descent uses a single learning rate across all features, it is more likely to overshoot minima along features which have steeper slopes along the loss surface than for features which have flatter slopes along the loss surface. ![](https://i.stack.imgur.com/5jBJL.png)\n",
        "> - If features are large, then the magnitude of the gradient is larger. This may cause the descent algorithm to overshoot minima of the loss function. It is better in this situation to have smaller features with a larger learning rate (as the learning rate can be tuned more easily).\n",
        "\n",
        "\n",
        "\n",
        "It is important to note - other methods have been used to mitigate some of maleffects of gradient descent here. For example - Adam is designed to have different learning rates per each feature, thus avoiding the issue in the first bullet point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFpWtNvJ7p0k"
      },
      "source": [
        "### Data Augmentation\n",
        "\n",
        "Data augmentation is a image preprocessing step that is used to combat **overfitting** in a convolutional neural network (and more generally any image based machine learning model).\n",
        "\n",
        "We'll discuss overfitting in depth next week, but for now let's motivate what it means for a model to overfit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYSfc3qZ8HVZ"
      },
      "source": [
        "Consider the case study from [this popular research paper](https://arxiv.org/pdf/1807.04975.pdf). In the paper, the researchers trained a computer vision model to recognize entities (such as a cow üêÆ).\n",
        "\n",
        "The authors then tried to extend their model to novel situations - such as identifying a cow on the beach (üêÆ üèùÔ∏è). For each of 3 situations, the authors, compared the top 5 most likely classes as per the model's predictions, and reported the results below (image reproduced from Figure 1 in paper).\n",
        "\n",
        "![Image of machine learning model training results for a cow in a pasture vs a cow on a beach.](https://miro.medium.com/v2/resize:fit:1400/0*EWVAp3dh3d5YwsGn.png)\n",
        "\n",
        "The figure above shows a clear trend:\n",
        "- for cows that appear in pastures, which is a common scenario, the model is very confident that the image contains a cow (99% confident)\n",
        "- for cows that appear on a beach, the model has no clue that the image contains a cow\n",
        "\n",
        "Why is this happening? **Overfitting**! The model in paper was trained _only_ on data that contained cows on pastures (as most cows usually do live on pastures). However as a result, the model learned to associate _pastures_ with _cows_ - which is a noisy trend! While cows may certainly tend to live on pastures, a cow need not live on a pasture to be a cow.\n",
        "\n",
        "In other words, the model fit _too closely_ to noisy patterns in the training dataset (the presence of pastures for cows) and learned relationships that didn't generalize well to new data (like cows on the beach). We'd like to avoid such behaviors in our models since it prevents our model from predicting unseen data correctly - which is why we want to tackle overfitting.\n",
        "\n",
        "**KEY üîë**: It's easy to write this off as a strange edge case, but overfitting tends to be _extremely common_ for models with high complexity (which we say have  **high variance**) - such as convolutional neural networks. These are really powerful models that very easily overfit to training data unless you have particularly clean data.\n",
        "\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFaULAlQ8Tl2"
      },
      "source": [
        "So how do we prevent overfitting? There are multiple solutions (many of which we'll discuss next week), but one way is to create a dataset that has fewer noisy trends. One way to do so is to _augment_ the data in such a way as to mitigate noisy trends. We call this **data augmentation**.\n",
        "\n",
        "The sections below will list a few popular data augmentation methods that you may consider. That said, there are _tons_ of potential data augmentations you should explore [here](https://pytorch.org/vision/stable/transforms.html#v2-api-reference-recommended)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmJDgCAa9wby"
      },
      "source": [
        "#### Random Crop\n",
        "\n",
        "This method involves randomly cropping subsets of the image. If you suspect that your model is learning a position bias (where an object being in a certain part of an image is influencing your prediction), cropping can help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "i9ft3No38TT2",
        "outputId": "1643486b-1e73-41d6-f547-40aaaf0d36ee"
      },
      "outputs": [],
      "source": [
        "cropper = v2.RandomCrop(size=(224, 224))\n",
        "plot(image, cropper(image), cropper(image))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms0uu5BiD1fe"
      },
      "source": [
        "Note how different calls to the `cropper` function lead to different parts of the image being cropped. This randomness in the cropping makes it harder for a model trained on this data to overfit to location information in the image. As it's harder to overfit to location information, the model can ideally learn more generic patterns in the image that correspond to the class we want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJl2xp2FX8C"
      },
      "source": [
        "#### Random Jitter\n",
        "\n",
        "Similar to random cropping, random jitter randomly shifts the color of each pixel in an image. Random jitter may be useful when you suspect that your model is overfitting to noisy color trends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "lpOFuBQ8Fx1z",
        "outputId": "d133c304-b6bc-4ba7-f1a2-e55d69c2c18c"
      },
      "outputs": [],
      "source": [
        "jitter = v2.ColorJitter(brightness=.5, hue=.3)\n",
        "plot(image, jitter(image), jitter(image))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaeEJPC2GVvn"
      },
      "source": [
        "#### Gaussian Blur\n",
        "\n",
        "Gaussian Blur adds blur to the image. This can be useful to smooth out image features and \"simplify\" the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "Ds0SSnVuGkp-",
        "outputId": "91d91212-701d-4520-e671-0690fa5405df"
      },
      "outputs": [],
      "source": [
        "blurrer = v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.))\n",
        "plot(image, blurrer(image))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6dR2HlOGwiO"
      },
      "source": [
        "#### Random Horizontal and Vertical Flip\n",
        "\n",
        "Random Horizontal and Vertical Flip flip the input image randomly with some probability. Like random cropping, this can help if you think your model is learning to overfit to the position of certain patterns in the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "NKRIuP6hHHoM",
        "outputId": "2e31625b-6885-4d43-ea30-5286276eed01"
      },
      "outputs": [],
      "source": [
        "hflipper = v2.RandomHorizontalFlip(p=1)\n",
        "vflipper = v2.RandomVerticalFlip(p=1)\n",
        "plot(image, hflipper(image), vflipper(image))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2aDiY8g9UlH"
      },
      "source": [
        "### Feature Selection\n",
        "\n",
        "Feature selections method are less common in modern computer vision due to properties of CNNs, but are certainly worth trying! In these methods, we will explicitly define ways to focus on certain subsets of pixels of the original image so our model can use specific extra information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J7d8aMo9ito"
      },
      "source": [
        "#### Edge Detection\n",
        "\n",
        "Edge detection is a method that isolates all the _edges_ in an image. Intuitively, edges contain lots of useful identifying information in an image - isolating this can potentially help our model make more accurate predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "CLHIVSK0Hrpn",
        "outputId": "3d53c892-f8c7-4722-d42a-cc9979a3406f"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "tight = torch.from_numpy(cv2.Canny(image.permute(1,2,0).numpy(), 240, 250)).view(1, 500, 500)\n",
        "plot(image, tight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxDYgZK_IVhf"
      },
      "source": [
        "As we see, the image on the right contains the image of the dog, but with only the edges isolated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKQ0dgK7w2TJ"
      },
      "source": [
        "## Implementing Preprocessing Methods\n",
        "\n",
        "This section will focus on _how_ image preprocessing is implemented when training a convolutional neural network. For this section, you may want to review the [Custom Dataset Walkthrough](https://github.com/MichiganDataScienceTeam/W24-RvF/blob/main/notebooks/dataset.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoJb9gt8KMBw"
      },
      "source": [
        "### CIFAR-10 Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo4ehLUoJoVA"
      },
      "source": [
        "In most cases, image preprocessing in PyTorch is done inside the `Dataset` class by a dedicated preprocess function.\n",
        "\n",
        "As an example, let's walkthrough creating an image preprocessing function for the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuaWKQvSw8OV",
        "outputId": "c7585330-5c5a-4529-ad1e-b7311383819b"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=\"./private\", train=True, download=True, transform=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnhezzV8KeUJ"
      },
      "source": [
        "Notice how the dataset above has `transform=None`. This tells PyTorch that there should be no preprocessing applied. However let's say we _do_ want to apply some preprocessing transformations:\n",
        "1. Normalize the images using the per-channel mean and standard deviation\n",
        "2. Apply Random cropping and Gaussian Blur to each image returned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92estKG6K9dS"
      },
      "source": [
        "The first step in this process is calculating the per-channel mean and standard deviation. The code below will do that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9PwaXUnKco3"
      },
      "outputs": [],
      "source": [
        "mean = torch.zeros((3,))\n",
        "variance = torch.zeros((3,))\n",
        "tensor_converter = v2.ToTensor()\n",
        "\n",
        "for image, _ in train_dataset:\n",
        "    mean += tensor_converter(image).mean(dim=(1, 2))\n",
        "mean /= len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyffgQ5AMW0_"
      },
      "outputs": [],
      "source": [
        "for image, _ in train_dataset:\n",
        "    image = tensor_converter(image)\n",
        "    variance += ((image - mean.view(3, 1, 1))**2).mean(dim=(1,2))\n",
        "\n",
        "std = torch.sqrt(variance / len(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGQWowWqMi84",
        "outputId": "25392fe2-ba94-49aa-ee55-91fe99147219"
      },
      "outputs": [],
      "source": [
        "mean, std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HM_YswGM2yW"
      },
      "source": [
        "Now we can create our preprocessor! This can be created using `torchvision.transforms.Compose`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAUE0egGM5-V"
      },
      "outputs": [],
      "source": [
        "def add_edge_features(image):\n",
        "    tight = torch.from_numpy(cv2.Canny(image.permute(1,2,0).numpy(), 240, 250)).view(1, 500, 500)\n",
        "    return torch.dstack([image, tight]) # Add an extra channel for edges\n",
        "\n",
        "\n",
        "tensor_converter = v2.Compose([ # Step 0: Convert from PIL Image to Torch Tensor\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True)\n",
        "])\n",
        "normalizer = v2.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2532, 0.2496, 0.2681]) # Step 1. Normalize the image\n",
        "cropper = v2.RandomCrop(size=(28, 28))\n",
        "blurrer = v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.))\n",
        "\n",
        "preprocessor = v2.Compose([\n",
        "    tensor_converter,\n",
        "    normalizer,\n",
        "    cropper,\n",
        "    blurrer\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI7Fuy2QOX6y"
      },
      "source": [
        "We can now recreate our dataset with this preprocessor function applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOrwmIIVOIrY",
        "outputId": "1ff10b43-391b-49a7-a19a-a2c6fc63051a"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.CIFAR10(root=\"./private\", train=True, download=True, transform=preprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNL-zxVbOeDP"
      },
      "source": [
        "If we try and view a few images now, we will see that they have been preprocessed as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "qhojm_ReOc0o",
        "outputId": "b07faafc-257d-447e-8761-2dff34878f56"
      },
      "outputs": [],
      "source": [
        "plot(train_dataset[0][0], train_dataset[0][0], train_dataset[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3vxj1iCPI5Q"
      },
      "source": [
        "## Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwN37DK9PLjW"
      },
      "source": [
        "Now it's your turn! Experiment with different preprocessing steps with the RvF dataset and see what preprocessing steps lead to the best model performance!\n",
        "\n",
        "If you are using the [starter code](https://github.com/MichiganDataScienceTeam/W24-RvF/tree/main/starter_code), you can specify your preprocessor as an input to the `get_loaders()` function. See the starter code [workbook](https://github.com/MichiganDataScienceTeam/W24-RvF/blob/main/starter_code/workbook.ipynb) for an example."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2jq1piSLwyxj",
        "7zbwnU1Aur1v",
        "aFpWtNvJ7p0k",
        "hmJDgCAa9wby",
        "bKJl2xp2FX8C",
        "KaeEJPC2GVvn",
        "s6dR2HlOGwiO",
        "TKQ0dgK7w2TJ",
        "t3vxj1iCPI5Q"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
